{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PqH5_efye0A7",
        "outputId": "f7fd9a2b-bfd8-4bd3-f31e-fdbe53c4dad4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "\nAll the 243 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n243 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py\", line 662, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/ensemble/_forest.py\", line 360, in fit\n    X, y = validate_data(\n           ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 2961, in validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 1387, in check_X_y\n    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 1397, in _check_y\n    y = check_array(\n        ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 1107, in check_array\n    _assert_all_finite(\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input y contains NaN.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2318382901.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# 11. GridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0msearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best parameters:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1571\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    999\u001b[0m                     )\n\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m                 \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m                 \u001b[0;31m# For callable self.scoring, the return type is only know after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             )\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \nAll the 243 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n243 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py\", line 662, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/ensemble/_forest.py\", line 360, in fit\n    X, y = validate_data(\n           ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 2961, in validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 1387, in check_X_y\n    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 1397, in _check_y\n    y = check_array(\n        ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 1107, in check_array\n    _assert_all_finite(\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input y contains NaN.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# 1. Load data\n",
        "\n",
        "df = pd.read_csv('used_cars_simplified.csv')\n",
        "\n",
        "# 2. Drop 'model' column to avoid high cardinality\n",
        "if 'model' in df.columns:\n",
        "    df = df.drop(columns=['model'])\n",
        "\n",
        "# 3. Clean 'milage' column: remove commas, 'mi.', and convert to float\n",
        "if 'milage' in df.columns:\n",
        "    df['milage'] = df['milage'].astype(str).str.replace(',', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.replace('mi.', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.replace('miles', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.replace('mi', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.strip()\n",
        "    df['milage'] = pd.to_numeric(df['milage'], errors='coerce')\n",
        "\n",
        "# 4. Clean other numeric columns if needed\n",
        "for col in ['model_year', 'engine_displacement_l', 'engine_cylinders', 'price']:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# 5. Define features and target\n",
        "X = df.drop('price', axis=1)\n",
        "y = df['price']\n",
        "\n",
        "# 6. Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 7. Feature lists (match your columns exactly)\n",
        "numeric_features = ['model_year', 'milage', 'engine_displacement_l', 'engine_cylinders']\n",
        "categorical_features = ['brand', 'fuel_type', 'transmission_type', 'ext_col', 'int_col', 'accident', 'clean_title']\n",
        "\n",
        "# 8. Preprocessing pipeline with SimpleImputer\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_features),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
        "    ]), categorical_features)\n",
        "])\n",
        "\n",
        "# 9. Full pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('regressor', RandomForestRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "# 10. Hyperparameter grid\n",
        "param_grid = {\n",
        "    'regressor__n_estimators': [50, 100, 200],\n",
        "    'regressor__max_depth': [None, 10, 20],\n",
        "    'regressor__min_samples_split': [2, 5, 10],\n",
        "    'regressor__min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# 11. GridSearchCV\n",
        "search = GridSearchCV(pipeline, param_grid, cv=3, scoring='r2', n_jobs=-1, verbose=2)\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters:\", search.best_params_)\n",
        "\n",
        "# 12. Evaluate on test set\n",
        "best_model = search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared: {r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# 1. Load data\n",
        "\n",
        "df = pd.read_csv('used_cars_simplified.csv')\n",
        "\n",
        "# 2. Drop 'model' column to avoid high cardinality\n",
        "if 'model' in df.columns:\n",
        "    df = df.drop(columns=['model'])\n",
        "\n",
        "# 3. Clean 'milage' column: remove commas, 'mi.', and convert to float\n",
        "if 'milage' in df.columns:\n",
        "    df['milage'] = df['milage'].astype(str).str.replace(',', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.replace('mi.', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.replace('miles', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.replace('mi', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.strip()\n",
        "    df['milage'] = pd.to_numeric(df['milage'], errors='coerce')\n",
        "\n",
        "# 4. Clean other numeric columns if needed\n",
        "for col in ['model_year', 'engine_displacement_l', 'engine_cylinders', 'price']:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# 5. Drop rows where price is missing\n",
        "# This is the key fix for your error!\n",
        "df = df.dropna(subset=['price'])\n",
        "\n",
        "# 6. Define features and target\n",
        "X = df.drop('price', axis=1)\n",
        "y = df['price']\n",
        "\n",
        "# 7. Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 8. Feature lists (match your columns exactly)\n",
        "numeric_features = ['model_year', 'milage', 'engine_displacement_l', 'engine_cylinders']\n",
        "categorical_features = ['brand', 'fuel_type', 'transmission_type', 'ext_col', 'int_col', 'accident', 'clean_title']\n",
        "\n",
        "# 9. Preprocessing pipeline with SimpleImputer\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_features),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
        "    ]), categorical_features)\n",
        "])\n",
        "\n",
        "# 10. Full pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('regressor', RandomForestRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "# 11. Hyperparameter grid\n",
        "param_grid = {\n",
        "    'regressor__n_estimators': [50, 100, 200],\n",
        "    'regressor__max_depth': [None, 10, 20],\n",
        "    'regressor__min_samples_split': [2, 5, 10],\n",
        "    'regressor__min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# 12. GridSearchCV\n",
        "search = GridSearchCV(pipeline, param_grid, cv=3, scoring='r2', n_jobs=-1, verbose=2)\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters:\", search.best_params_)\n",
        "\n",
        "# 13. Evaluate on test set\n",
        "best_model = search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "bTUMXHPhhLs3",
        "outputId": "35f23dc5-e586-4089-a6bb-6d1343f55764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-33243177.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# 7. Split data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# 8. Feature lists (match your columns exactly)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2851\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2852\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2482\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# 1. Load data\n",
        "\n",
        "df = pd.read_csv('used_cars_simplified.csv')\n",
        "\n",
        "# 2. Drop 'model' column to avoid high cardinality\n",
        "if 'model' in df.columns:\n",
        "    df = df.drop(columns=['model'])\n",
        "\n",
        "# 3. Clean 'milage' column: remove commas, 'mi.', and convert to float\n",
        "if 'milage' in df.columns:\n",
        "    df['milage'] = df['milage'].astype(str).str.replace(',', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.replace('mi.', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.replace('miles', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.replace('mi', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.strip()\n",
        "    df['milage'] = pd.to_numeric(df['milage'], errors='coerce')\n",
        "\n",
        "# 4. Clean 'price' column: remove '$', ',', and convert to float\n",
        "if 'price' in df.columns:\n",
        "    df['price'] = df['price'].astype(str).str.replace('$', '', regex=False)\n",
        "    df['price'] = df['price'].str.replace(',', '', regex=False)\n",
        "    df['price'] = df['price'].str.strip()\n",
        "    df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
        "\n",
        "# 5. Clean other numeric columns if needed\n",
        "for col in ['model_year', 'engine_displacement_l', 'engine_cylinders']:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# 6. Drop rows where price is missing\n",
        "# This is the key fix for your error!\n",
        "df = df.dropna(subset=['price'])\n",
        "\n",
        "# 7. Define features and target\n",
        "X = df.drop('price', axis=1)\n",
        "y = df['price']\n",
        "\n",
        "# 8. Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 9. Feature lists (match your columns exactly)\n",
        "numeric_features = ['model_year', 'milage', 'engine_displacement_l', 'engine_cylinders']\n",
        "categorical_features = ['brand', 'fuel_type', 'transmission_type', 'ext_col', 'int_col', 'accident', 'clean_title']\n",
        "\n",
        "# 10. Preprocessing pipeline with SimpleImputer\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_features),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
        "    ]), categorical_features)\n",
        "])\n",
        "\n",
        "# 11. Full pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('regressor', RandomForestRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "# 12. Hyperparameter grid\n",
        "param_grid = {\n",
        "    'regressor__n_estimators': [50, 100, 200],\n",
        "    'regressor__max_depth': [None, 10, 20],\n",
        "    'regressor__min_samples_split': [2, 5, 10],\n",
        "    'regressor__min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# 13. GridSearchCV\n",
        "search = GridSearchCV(pipeline, param_grid, cv=3, scoring='r2', n_jobs=-1, verbose=2)\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters:\", search.best_params_)\n",
        "\n",
        "# 14. Evaluate on test set\n",
        "best_model = search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "nDxeDufHhMSd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71055cf5-cb42-405e-ef65-e16d361df214"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# 1. Load data\n",
        "\n",
        "df = pd.read_csv('used_cars_simplified.csv')\n",
        "\n",
        "# 2. Drop 'model' column to avoid high cardinality\n",
        "if 'model' in df.columns:\n",
        "    df = df.drop(columns=['model'])\n",
        "\n",
        "# 3. Clean 'milage' column: remove commas, 'mi.', and convert to float\n",
        "if 'milage' in df.columns:\n",
        "    df['milage'] = df['milage'].astype(str).str.replace(',', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.replace('mi.', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.replace('miles', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.replace('mi', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.strip()\n",
        "    df['milage'] = pd.to_numeric(df['milage'], errors='coerce')\n",
        "\n",
        "# 4. Clean 'price' column: remove '$', ',', and convert to float\n",
        "if 'price' in df.columns:\n",
        "    df['price'] = df['price'].astype(str).str.replace('$', '', regex=False)\n",
        "    df['price'] = df['price'].str.replace(',', '', regex=False)\n",
        "    df['price'] = df['price'].str.strip()\n",
        "    df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
        "\n",
        "# 5. Clean other numeric columns if needed\n",
        "for col in ['model_year', 'engine_displacement_l', 'engine_cylinders']:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# 6. Drop rows where price is missing\n",
        "# This is the key fix for your error!\n",
        "df = df.dropna(subset=['price'])\n",
        "\n",
        "# 7. Define features and target\n",
        "X = df.drop('price', axis=1)\n",
        "y = df['price']\n",
        "\n",
        "# 8. Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 9. Feature lists (match your columns exactly)\n",
        "numeric_features = ['model_year', 'milage', 'engine_displacement_l', 'engine_cylinders']\n",
        "categorical_features = ['brand', 'fuel_type', 'transmission_type', 'ext_col', 'int_col', 'accident', 'clean_title']\n",
        "\n",
        "# 10. Preprocessing pipeline with SimpleImputer\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_features),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
        "    ]), categorical_features)\n",
        "])\n",
        "\n",
        "# 11. Full pipeline with Linear Regression\n",
        "pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# 12. Train\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# 13. Evaluate on test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared: {r2:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrUSb5lQhxzb",
        "outputId": "46d1b1b6-8790-4ab9-f46a-db9f126bb457"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error: 24456.05\n",
            "Mean Squared Error: 19083118060.66\n",
            "R-squared: 0.0664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [0, 3, 4] during transform. These unknown categories will be encoded as all zeros\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# 1. Load data\n",
        "df = pd.read_csv('used_cars_simplified.csv')\n",
        "\n",
        "# 2. Drop 'model' column to avoid high cardinality\n",
        "if 'model' in df.columns:\n",
        "    df = df.drop(columns=['model'])\n",
        "\n",
        "# 3. Clean 'milage' column: remove commas, 'mi.', and convert to float\n",
        "if 'milage' in df.columns:\n",
        "    df['milage'] = df['milage'].astype(str).str.replace(',', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.replace('mi.', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.replace('miles', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.replace('mi', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.strip()\n",
        "    df['milage'] = pd.to_numeric(df['milage'], errors='coerce')\n",
        "\n",
        "# 4. Clean 'price' column: remove '$', ',', and convert to float\n",
        "if 'price' in df.columns:\n",
        "    df['price'] = df['price'].astype(str).str.replace('$', '', regex=False)\n",
        "    df['price'] = df['price'].str.replace(',', '', regex=False)\n",
        "    df['price'] = df['price'].str.strip()\n",
        "    df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
        "\n",
        "# 5. Clean other numeric columns if needed\n",
        "for col in ['model_year', 'engine_displacement_l', 'engine_cylinders']:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# 6. Drop rows where price is missing\n",
        "df = df.dropna(subset=['price'])\n",
        "\n",
        "# 7. Define features and target\n",
        "X = df.drop('price', axis=1)\n",
        "y = df['price']\n",
        "\n",
        "# 8. Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 9. Feature lists (match your columns exactly)\n",
        "numeric_features = ['model_year', 'milage', 'engine_displacement_l', 'engine_cylinders']\n",
        "categorical_features = ['brand', 'fuel_type', 'transmission_type', 'ext_col', 'int_col', 'accident', 'clean_title']\n",
        "\n",
        "# 10. Preprocessing pipeline with SimpleImputer\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_features),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
        "    ]), categorical_features)\n",
        "])\n",
        "\n",
        "# 11. Full pipeline with Random Forest\n",
        "pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('regressor', RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42))\n",
        "])\n",
        "\n",
        "# 12. Train\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# 13. Evaluate on test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "g6J5K809jEA9",
        "outputId": "af240d3b-a9a0-4052-fdc4-be8ec23b653c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error: 19175.72\n",
            "Mean Squared Error: 17158710102.61\n",
            "R-squared: 0.1605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [0, 3, 4] during transform. These unknown categories will be encoded as all zeros\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# 1. Load data\n",
        "df = pd.read_csv('used_cars_simplified.csv')\n",
        "\n",
        "# 2. Drop 'model' column to avoid high cardinality\n",
        "if 'model' in df.columns:\n",
        "    df = df.drop(columns=['model'])\n",
        "\n",
        "# 3. Clean 'milage' column: remove commas, 'mi.', and convert to float\n",
        "if 'milage' in df.columns:\n",
        "    df['milage'] = df['milage'].astype(str).str.replace(',', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.replace('mi.', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.replace('miles', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.replace('mi', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.strip()\n",
        "    df['milage'] = pd.to_numeric(df['milage'], errors='coerce')\n",
        "\n",
        "# 4. Clean 'price' column: remove '$', ',', and convert to float\n",
        "if 'price' in df.columns:\n",
        "    df['price'] = df['price'].astype(str).str.replace('$', '', regex=False)\n",
        "    df['price'] = df['price'].str.replace(',', '', regex=False)\n",
        "    df['price'] = df['price'].str.strip()\n",
        "    df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
        "\n",
        "# 5. Clean other numeric columns if needed\n",
        "for col in ['model_year', 'engine_displacement_l', 'engine_cylinders']:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# 6. Drop rows where price is missing\n",
        "df = df.dropna(subset=['price'])\n",
        "\n",
        "# 7. Define features and target\n",
        "X = df.drop('price', axis=1)\n",
        "y = df['price']\n",
        "\n",
        "# 8. Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 9. Feature lists (match your columns exactly)\n",
        "numeric_features = ['model_year', 'milage', 'engine_displacement_l', 'engine_cylinders']\n",
        "categorical_features = ['brand', 'fuel_type', 'transmission_type', 'ext_col', 'int_col', 'accident', 'clean_title']\n",
        "\n",
        "# 10. Preprocessing pipeline with SimpleImputer\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_features),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
        "    ]), categorical_features)\n",
        "])\n",
        "\n",
        "# 11. Full pipeline with XGBoost\n",
        "pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('regressor', XGBRegressor(n_estimators=300, max_depth=8, learning_rate=0.1, random_state=42, n_jobs=-1))\n",
        "])\n",
        "\n",
        "# 12. Train\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# 13. Evaluate on test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "qIX1TQyajn2h",
        "outputId": "61c1b619-e602-4887-921b-370a63686641",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error: 18090.25\n",
            "Mean Squared Error: 16812980224.00\n",
            "R-squared: 0.1774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [0, 3, 4] during transform. These unknown categories will be encoded as all zeros\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from xgboost import XGBRegressor\n",
        "import optuna\n",
        "\n",
        "# 1. Load data\n",
        "df = pd.read_csv('used_cars_simplified.csv')\n",
        "\n",
        "# 2. Drop 'model' column to avoid high cardinality\n",
        "if 'model' in df.columns:\n",
        "    df = df.drop(columns=['model'])\n",
        "\n",
        "# 3. Clean 'milage' column: remove commas, 'mi.', and convert to float\n",
        "if 'milage' in df.columns:\n",
        "    df['milage'] = df['milage'].astype(str).str.replace(',', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.replace('mi.', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.replace('miles', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.replace('mi', '', regex=False)\n",
        "    df['milage'] = df['milage'].str.strip()\n",
        "    df['milage'] = pd.to_numeric(df['milage'], errors='coerce')\n",
        "\n",
        "# 4. Clean 'price' column: remove '$', ',', and convert to float\n",
        "if 'price' in df.columns:\n",
        "    df['price'] = df['price'].astype(str).str.replace('$', '', regex=False)\n",
        "    df['price'] = df['price'].str.replace(',', '', regex=False)\n",
        "    df['price'] = df['price'].str.strip()\n",
        "    df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
        "\n",
        "# 5. Clean other numeric columns if needed\n",
        "for col in ['model_year', 'engine_displacement_l', 'engine_cylinders']:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# 6. Drop rows where price is missing\n",
        "df = df.dropna(subset=['price'])\n",
        "\n",
        "# 7. Define features and target\n",
        "X = df.drop('price', axis=1)\n",
        "y = df['price']\n",
        "\n",
        "# 8. Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 9. Feature lists (match your columns exactly)\n",
        "numeric_features = ['model_year', 'milage', 'engine_displacement_l', 'engine_cylinders']\n",
        "categorical_features = ['brand', 'fuel_type', 'transmission_type', 'ext_col', 'int_col', 'accident', 'clean_title']\n",
        "\n",
        "# 10. Preprocessing pipeline with SimpleImputer\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_features),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
        "    ]), categorical_features)\n",
        "])\n",
        "\n",
        "# 11. Optuna objective function\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 600),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 2.0),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 2.0),\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1\n",
        "    }\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocess', preprocessor),\n",
        "        ('regressor', XGBRegressor(**params))\n",
        "    ])\n",
        "    scores = cross_val_score(pipeline, X_train, y_train, cv=3, scoring='r2', n_jobs=-1)\n",
        "    return np.mean(scores)\n",
        "\n",
        "# 12. Run Optuna study\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=40)\n",
        "print('Best trial:', study.best_trial.params)\n",
        "\n",
        "# 13. Train final model with best params\n",
        "best_params = study.best_trial.params\n",
        "pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('regressor', XGBRegressor(**best_params))\n",
        "])\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# 14. Evaluate on test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "uOUDY421j950",
        "outputId": "dbf2987a-40fb-46a8-cfe5-368cf0c66f63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-09-30 16:59:54,902] A new study created in memory with name: no-name-b77518ed-c868-456d-84d6-c2e0553e43b8\n",
            "[I 2025-09-30 17:00:22,865] Trial 0 finished with value: 0.7541163563728333 and parameters: {'n_estimators': 414, 'max_depth': 11, 'learning_rate': 0.051094606309504416, 'subsample': 0.6001589330689612, 'colsample_bytree': 0.7227475159945312, 'reg_alpha': 1.6043095369720526, 'reg_lambda': 1.9129582264760636}. Best is trial 0 with value: 0.7541163563728333.\n",
            "[I 2025-09-30 17:00:25,438] Trial 1 finished with value: 0.7354361613591512 and parameters: {'n_estimators': 401, 'max_depth': 7, 'learning_rate': 0.2627908282229473, 'subsample': 0.9784047423093085, 'colsample_bytree': 0.9907066604162295, 'reg_alpha': 0.3773675371737497, 'reg_lambda': 0.5938062059337341}. Best is trial 0 with value: 0.7541163563728333.\n",
            "[I 2025-09-30 17:00:29,240] Trial 2 finished with value: 0.7572028636932373 and parameters: {'n_estimators': 338, 'max_depth': 9, 'learning_rate': 0.1331216586928483, 'subsample': 0.8156311967065132, 'colsample_bytree': 0.7969983287998278, 'reg_alpha': 0.8138093446399328, 'reg_lambda': 0.9554313522730731}. Best is trial 2 with value: 0.7572028636932373.\n",
            "[I 2025-09-30 17:00:30,763] Trial 3 finished with value: 0.7583415110905966 and parameters: {'n_estimators': 565, 'max_depth': 5, 'learning_rate': 0.1452867683897297, 'subsample': 0.6925896817393284, 'colsample_bytree': 0.5986204906768299, 'reg_alpha': 0.5130550204326982, 'reg_lambda': 0.9685088084175377}. Best is trial 3 with value: 0.7583415110905966.\n",
            "[I 2025-09-30 17:00:38,530] Trial 4 finished with value: 0.7539942463239034 and parameters: {'n_estimators': 513, 'max_depth': 13, 'learning_rate': 0.06617018489729462, 'subsample': 0.8247575935122737, 'colsample_bytree': 0.6028703025265925, 'reg_alpha': 1.4012624173908612, 'reg_lambda': 0.32147246567006715}. Best is trial 3 with value: 0.7583415110905966.\n",
            "[I 2025-09-30 17:00:44,489] Trial 5 finished with value: 0.7262263894081116 and parameters: {'n_estimators': 297, 'max_depth': 12, 'learning_rate': 0.2998563427514787, 'subsample': 0.6050584949646937, 'colsample_bytree': 0.7638762571382863, 'reg_alpha': 0.3419376670606129, 'reg_lambda': 1.1414409240269738}. Best is trial 3 with value: 0.7583415110905966.\n",
            "[I 2025-09-30 17:00:54,514] Trial 6 finished with value: 0.7487794160842896 and parameters: {'n_estimators': 426, 'max_depth': 15, 'learning_rate': 0.12191586575372197, 'subsample': 0.6849163085809862, 'colsample_bytree': 0.7443447441650153, 'reg_alpha': 1.8149893423375574, 'reg_lambda': 0.8751289703131733}. Best is trial 3 with value: 0.7583415110905966.\n",
            "[I 2025-09-30 17:00:55,010] Trial 7 finished with value: 0.7187033891677856 and parameters: {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.04938661609334432, 'subsample': 0.5387344747100933, 'colsample_bytree': 0.8426178108031435, 'reg_alpha': 0.7991074583077125, 'reg_lambda': 0.63816538215938}. Best is trial 3 with value: 0.7583415110905966.\n",
            "[I 2025-09-30 17:00:58,303] Trial 8 finished with value: 0.7586779991785685 and parameters: {'n_estimators': 571, 'max_depth': 7, 'learning_rate': 0.16862253652109957, 'subsample': 0.6730937670842396, 'colsample_bytree': 0.650229567599641, 'reg_alpha': 1.135571371087431, 'reg_lambda': 1.7640983979949538}. Best is trial 8 with value: 0.7586779991785685.\n",
            "[I 2025-09-30 17:01:11,539] Trial 9 finished with value: 0.7455215652783712 and parameters: {'n_estimators': 568, 'max_depth': 14, 'learning_rate': 0.022092638895759603, 'subsample': 0.9155411267582336, 'colsample_bytree': 0.9441846343170586, 'reg_alpha': 0.2660120504500507, 'reg_lambda': 1.9810484173540208}. Best is trial 8 with value: 0.7586779991785685.\n",
            "[I 2025-09-30 17:01:12,481] Trial 10 finished with value: 0.7380907932917277 and parameters: {'n_estimators': 243, 'max_depth': 7, 'learning_rate': 0.22490005982596584, 'subsample': 0.7624632540997038, 'colsample_bytree': 0.5024898325497336, 'reg_alpha': 1.092692895666496, 'reg_lambda': 1.4483013911414802}. Best is trial 8 with value: 0.7586779991785685.\n",
            "[I 2025-09-30 17:01:13,603] Trial 11 finished with value: 0.77194611231486 and parameters: {'n_estimators': 591, 'max_depth': 4, 'learning_rate': 0.19305952787143696, 'subsample': 0.6839290445386818, 'colsample_bytree': 0.6276607589980941, 'reg_alpha': 1.138599239478727, 'reg_lambda': 1.4286706184729667}. Best is trial 11 with value: 0.77194611231486.\n",
            "[I 2025-09-30 17:01:14,306] Trial 12 finished with value: 0.7808722456296285 and parameters: {'n_estimators': 489, 'max_depth': 3, 'learning_rate': 0.19835034506233776, 'subsample': 0.6730991390060816, 'colsample_bytree': 0.6390499391469543, 'reg_alpha': 1.2321664650617599, 'reg_lambda': 1.5151252351368312}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:14,976] Trial 13 finished with value: 0.7585171262423197 and parameters: {'n_estimators': 489, 'max_depth': 3, 'learning_rate': 0.20328380357625386, 'subsample': 0.509988709217028, 'colsample_bytree': 0.5099141477549849, 'reg_alpha': 1.3983004781953208, 'reg_lambda': 1.4641048866810469}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:16,224] Trial 14 finished with value: 0.7670806646347046 and parameters: {'n_estimators': 487, 'max_depth': 5, 'learning_rate': 0.2021321841950945, 'subsample': 0.7572340776432749, 'colsample_bytree': 0.6684058926810224, 'reg_alpha': 1.9992922805177669, 'reg_lambda': 1.5087588095422815}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:17,045] Trial 15 finished with value: 0.7573259870211283 and parameters: {'n_estimators': 591, 'max_depth': 3, 'learning_rate': 0.24297501024312723, 'subsample': 0.626698604530053, 'colsample_bytree': 0.5799321991884164, 'reg_alpha': 0.0206879537698047, 'reg_lambda': 1.263679621993426}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:21,442] Trial 16 finished with value: 0.7656895319620768 and parameters: {'n_estimators': 503, 'max_depth': 9, 'learning_rate': 0.16717262375212466, 'subsample': 0.8311826798037707, 'colsample_bytree': 0.6760150688261636, 'reg_alpha': 1.3496757980859926, 'reg_lambda': 1.6289736213531236}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:23,208] Trial 17 finished with value: 0.7581116954485575 and parameters: {'n_estimators': 451, 'max_depth': 5, 'learning_rate': 0.1919838202667677, 'subsample': 0.721314487980056, 'colsample_bytree': 0.5687606736879356, 'reg_alpha': 0.8715164056463516, 'reg_lambda': 1.2613192294452078}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:24,037] Trial 18 finished with value: 0.7511047919591268 and parameters: {'n_estimators': 198, 'max_depth': 6, 'learning_rate': 0.10405361800697845, 'subsample': 0.6300648484085938, 'colsample_bytree': 0.8677372760327635, 'reg_alpha': 1.173646671188763, 'reg_lambda': 0.10602562204135346}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:24,609] Trial 19 finished with value: 0.7541550795237223 and parameters: {'n_estimators': 363, 'max_depth': 3, 'learning_rate': 0.2723140273677953, 'subsample': 0.7834493775110585, 'colsample_bytree': 0.6517299090107648, 'reg_alpha': 0.6406329192725022, 'reg_lambda': 1.727365159485347}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:28,747] Trial 20 finished with value: 0.7494273583094279 and parameters: {'n_estimators': 537, 'max_depth': 10, 'learning_rate': 0.09285163553499326, 'subsample': 0.5621769270131374, 'colsample_bytree': 0.5472871271305948, 'reg_alpha': 1.6335769221868408, 'reg_lambda': 1.2748101432333891}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:29,985] Trial 21 finished with value: 0.7603173653284708 and parameters: {'n_estimators': 469, 'max_depth': 5, 'learning_rate': 0.1995183951523141, 'subsample': 0.7392228071100795, 'colsample_bytree': 0.6944756981620244, 'reg_alpha': 1.9912300678951185, 'reg_lambda': 1.4959805478043087}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:30,921] Trial 22 finished with value: 0.7554552356402079 and parameters: {'n_estimators': 515, 'max_depth': 4, 'learning_rate': 0.22471799821335284, 'subsample': 0.8754213471676444, 'colsample_bytree': 0.6212643457670713, 'reg_alpha': 1.6836607564929444, 'reg_lambda': 1.4839783353035803}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:33,118] Trial 23 finished with value: 0.7661575476328532 and parameters: {'n_estimators': 599, 'max_depth': 6, 'learning_rate': 0.1757559174172396, 'subsample': 0.6680375349024279, 'colsample_bytree': 0.7075179193930564, 'reg_alpha': 1.2915715269094554, 'reg_lambda': 1.6510283397948216}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:34,600] Trial 24 finished with value: 0.7576903899510702 and parameters: {'n_estimators': 537, 'max_depth': 4, 'learning_rate': 0.22330423859499585, 'subsample': 0.7287370661684187, 'colsample_bytree': 0.6542786934383359, 'reg_alpha': 1.007169507011523, 'reg_lambda': 1.1440005596411744}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:38,919] Trial 25 finished with value: 0.7547940214474996 and parameters: {'n_estimators': 459, 'max_depth': 8, 'learning_rate': 0.18843265005653936, 'subsample': 0.787432009309069, 'colsample_bytree': 0.637714897097554, 'reg_alpha': 1.9412339450185734, 'reg_lambda': 1.3761591292588105}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:39,906] Trial 26 finished with value: 0.7568410833676656 and parameters: {'n_estimators': 374, 'max_depth': 3, 'learning_rate': 0.2502685708645548, 'subsample': 0.6455544327345147, 'colsample_bytree': 0.7828620881260742, 'reg_alpha': 1.5402841979713893, 'reg_lambda': 1.8058597327665518}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:42,127] Trial 27 finished with value: 0.7674876054128011 and parameters: {'n_estimators': 542, 'max_depth': 6, 'learning_rate': 0.1526357506211123, 'subsample': 0.702692451460493, 'colsample_bytree': 0.6801291974974515, 'reg_alpha': 1.7909842131796754, 'reg_lambda': 1.5759424206778831}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:43,838] Trial 28 finished with value: 0.7560555934906006 and parameters: {'n_estimators': 547, 'max_depth': 6, 'learning_rate': 0.1477997778405422, 'subsample': 0.596123096227053, 'colsample_bytree': 0.5594125479137719, 'reg_alpha': 1.7642564793741649, 'reg_lambda': 0.7771785358707529}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:44,960] Trial 29 finished with value: 0.7622589270273844 and parameters: {'n_estimators': 598, 'max_depth': 4, 'learning_rate': 0.11903726611027848, 'subsample': 0.7158229346158494, 'colsample_bytree': 0.7173265370370198, 'reg_alpha': 1.5232896978345654, 'reg_lambda': 1.8749227303445244}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:48,058] Trial 30 finished with value: 0.7439005970954895 and parameters: {'n_estimators': 440, 'max_depth': 8, 'learning_rate': 0.15664465198056482, 'subsample': 0.5634967780593055, 'colsample_bytree': 0.7399318271042294, 'reg_alpha': 1.2550138960716564, 'reg_lambda': 1.613306346311887}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:50,224] Trial 31 finished with value: 0.74601678053538 and parameters: {'n_estimators': 484, 'max_depth': 5, 'learning_rate': 0.21126072653884767, 'subsample': 0.6917800232502931, 'colsample_bytree': 0.6782807844801964, 'reg_alpha': 1.8703233403386392, 'reg_lambda': 1.1421867228521254}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:51,697] Trial 32 finished with value: 0.7624454696973165 and parameters: {'n_estimators': 417, 'max_depth': 6, 'learning_rate': 0.18579584854765707, 'subsample': 0.7661568519259963, 'colsample_bytree': 0.6180389807145493, 'reg_alpha': 1.7520385145558168, 'reg_lambda': 1.5809724628168285}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:52,744] Trial 33 finished with value: 0.7560442288716634 and parameters: {'n_estimators': 526, 'max_depth': 4, 'learning_rate': 0.24136545491683528, 'subsample': 0.6499876513758189, 'colsample_bytree': 0.677490332048304, 'reg_alpha': 1.4768812666650724, 'reg_lambda': 1.3252248481190825}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:53,504] Trial 34 finished with value: 0.7713443835576376 and parameters: {'n_estimators': 317, 'max_depth': 5, 'learning_rate': 0.1343659016363619, 'subsample': 0.6984784070275548, 'colsample_bytree': 0.533505410000236, 'reg_alpha': 0.9377244169125074, 'reg_lambda': 1.8574359151624615}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:54,712] Trial 35 finished with value: 0.7600574096043905 and parameters: {'n_estimators': 314, 'max_depth': 7, 'learning_rate': 0.1399133755686154, 'subsample': 0.7058768151442688, 'colsample_bytree': 0.5364863035970959, 'reg_alpha': 0.9487592215002068, 'reg_lambda': 1.996578107381014}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:56,713] Trial 36 finished with value: 0.7448471387227377 and parameters: {'n_estimators': 260, 'max_depth': 10, 'learning_rate': 0.09772123079229711, 'subsample': 0.6576249320330839, 'colsample_bytree': 0.5969092200091779, 'reg_alpha': 0.6742981774695684, 'reg_lambda': 1.8200177122371664}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:57,268] Trial 37 finished with value: 0.7719412048657736 and parameters: {'n_estimators': 394, 'max_depth': 3, 'learning_rate': 0.12592563051538164, 'subsample': 0.6009804228073966, 'colsample_bytree': 0.5239016149659643, 'reg_alpha': 0.9921584817641919, 'reg_lambda': 1.7200809819907361}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:57,848] Trial 38 finished with value: 0.7671734690666199 and parameters: {'n_estimators': 401, 'max_depth': 3, 'learning_rate': 0.07269219897260296, 'subsample': 0.5982866902028275, 'colsample_bytree': 0.5184677291802164, 'reg_alpha': 1.054713286042486, 'reg_lambda': 1.720982962561279}. Best is trial 12 with value: 0.7808722456296285.\n",
            "[I 2025-09-30 17:01:58,452] Trial 39 finished with value: 0.7683602770169576 and parameters: {'n_estimators': 323, 'max_depth': 4, 'learning_rate': 0.12075837689940147, 'subsample': 0.5774893532939313, 'colsample_bytree': 0.5355984926475535, 'reg_alpha': 0.9106093987583793, 'reg_lambda': 1.8938996546139162}. Best is trial 12 with value: 0.7808722456296285.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial: {'n_estimators': 489, 'max_depth': 3, 'learning_rate': 0.19835034506233776, 'subsample': 0.6730991390060816, 'colsample_bytree': 0.6390499391469543, 'reg_alpha': 1.2321664650617599, 'reg_lambda': 1.5151252351368312}\n",
            "Mean Absolute Error: 18496.51\n",
            "Mean Squared Error: 17569193984.00\n",
            "R-squared: 0.1404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [0, 3, 4] during transform. These unknown categories will be encoded as all zeros\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from catboost import CatBoostRegressor\n",
        "import datetime\n",
        "\n",
        "# 1. Load data\n",
        "df = pd.read_csv('used_cars_simplified.csv')\n",
        "\n",
        "# 2. Drop 'model' column to avoid high cardinality\n",
        "if 'model' in df.columns:\n",
        "    df = df.drop(columns=['model'])\n",
        "\n",
        "# 3. Clean 'milage' column\n",
        "df['milage'] = df['milage'].astype(str).str.replace(',', '', regex=False)\n",
        "df['milage'] = df['milage'].str.replace('mi.', '', regex=False)\n",
        "df['milage'] = df['milage'].str.replace('miles', '', regex=False)\n",
        "df['milage'] = df['milage'].str.replace('mi', '', regex=False)\n",
        "df['milage'] = df['milage'].str.strip()\n",
        "df['milage'] = pd.to_numeric(df['milage'], errors='coerce')\n",
        "\n",
        "# 4. Clean 'price' column\n",
        "df['price'] = df['price'].astype(str).str.replace('$', '', regex=False)\n",
        "df['price'] = df['price'].str.replace(',', '', regex=False)\n",
        "df['price'] = df['price'].str.strip()\n",
        "df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
        "\n",
        "# 5. Clean other numeric columns\n",
        "for col in ['model_year', 'engine_displacement_l', 'engine_cylinders']:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# 6. Fill missing values in categorical columns with 'missing'\n",
        "categorical_cols = [col for col in df.columns if df[col].dtype == 'object']\n",
        "for col in categorical_cols:\n",
        "    df[col] = df[col].fillna('missing')\n",
        "\n",
        "# 7. Drop rows where price is missing and remove outliers\n",
        "price_q_low = df['price'].quantile(0.01)\n",
        "price_q_high = df['price'].quantile(0.99)\n",
        "milage_q_low = df['milage'].quantile(0.01)\n",
        "milage_q_high = df['milage'].quantile(0.99)\n",
        "df = df[(df['price'] >= price_q_low) & (df['price'] <= price_q_high)]\n",
        "df = df[(df['milage'] >= milage_q_low) & (df['milage'] <= milage_q_high)]\n",
        "df = df.dropna(subset=['price'])\n",
        "\n",
        "# 8. Feature engineering: car age\n",
        "current_year = datetime.datetime.now().year\n",
        "df['car_age'] = current_year - df['model_year']\n",
        "\n",
        "# 9. Log-transform the target\n",
        "df['price_log'] = np.log1p(df['price'])\n",
        "\n",
        "# 10. Define features and target\n",
        "X = df.drop(['price', 'price_log'], axis=1)\n",
        "y = df['price_log']\n",
        "\n",
        "# 11. Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 12. Categorical features for CatBoost\n",
        "categorical_features = [col for col in X.columns if X[col].dtype == 'object']\n",
        "\n",
        "# 13. Train CatBoost\n",
        "model = CatBoostRegressor(\n",
        "    iterations=1000,\n",
        "    learning_rate=0.05,\n",
        "    depth=8,\n",
        "    eval_metric='R2',  # Use R2 as the evaluation metric\n",
        "    random_seed=42,\n",
        "    cat_features=categorical_features,\n",
        "    verbose=100\n",
        ")\n",
        "model.fit(X_train, y_train, eval_set=(X_test, y_test), use_best_model=True)\n",
        "\n",
        "# 14. Evaluate on test set (convert back from log)\n",
        "y_pred_log = model.predict(X_test)\n",
        "y_pred = np.expm1(y_pred_log)\n",
        "y_true = np.expm1(y_test)\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "WRz_t-DClIuR",
        "outputId": "3846921d-3552-49c4-ede2-0c7b43ef64e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 0.0610500\ttest: 0.0585040\tbest: 0.0585040 (0)\ttotal: 98.5ms\tremaining: 1m 38s\n",
            "100:\tlearn: 0.8524421\ttest: 0.8140291\tbest: 0.8140757 (99)\ttotal: 2.87s\tremaining: 25.6s\n",
            "200:\tlearn: 0.8855146\ttest: 0.8292187\tbest: 0.8292187 (200)\ttotal: 4.16s\tremaining: 16.5s\n",
            "300:\tlearn: 0.9060949\ttest: 0.8362151\tbest: 0.8363954 (291)\ttotal: 5.5s\tremaining: 12.8s\n",
            "400:\tlearn: 0.9223016\ttest: 0.8394363\tbest: 0.8394724 (399)\ttotal: 7.61s\tremaining: 11.4s\n",
            "500:\tlearn: 0.9341599\ttest: 0.8418657\tbest: 0.8418903 (499)\ttotal: 9.59s\tremaining: 9.55s\n",
            "600:\tlearn: 0.9421979\ttest: 0.8429077\tbest: 0.8429594 (589)\ttotal: 11s\tremaining: 7.29s\n",
            "700:\tlearn: 0.9492305\ttest: 0.8441232\tbest: 0.8441429 (698)\ttotal: 12.4s\tremaining: 5.28s\n",
            "800:\tlearn: 0.9560746\ttest: 0.8447581\tbest: 0.8447581 (800)\ttotal: 13.8s\tremaining: 3.42s\n",
            "900:\tlearn: 0.9616897\ttest: 0.8455992\tbest: 0.8457361 (893)\ttotal: 15.1s\tremaining: 1.66s\n",
            "999:\tlearn: 0.9656903\ttest: 0.8460735\tbest: 0.8460735 (999)\ttotal: 16.6s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.8460735251\n",
            "bestIteration = 999\n",
            "\n",
            "Mean Absolute Error: 8094.08\n",
            "Mean Squared Error: 217404465.79\n",
            "R-squared: 0.7776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna xgboost"
      ],
      "metadata": {
        "id": "JiKwMNJOkHpw",
        "outputId": "46e9d82b-f6aa-4239-d6a2-cc24c031c686",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.5)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.16.5)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.9.0 optuna-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install catboost\n"
      ],
      "metadata": {
        "id": "pPCjVfCylTJs",
        "outputId": "bc732041-070d-4d5d-9b4e-8dbb84372931",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0NpFdwlSlTly"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}